# icrawler

Simple Python-based web crawler that downloads PDF files and converts web
pages to PDF. Provide seed URLs and the crawler will follow links on those
pages, downloading linked PDFs or rendering HTML pages to PDF files.

## Usage

```
python -m icrawler.crawler [--delay N] [--jitter M] <output_dir> <url1> [<url2> ...]
```

`--delay` sets the base pause between requests, and `--jitter` adds up to that
many random seconds so servers are not hammered. Install the dependencies from
`requirements.txt` before running.

## PBC Monitor Quick Start

`icrawler.pbc_monitor` loads tasks from `pbc_config.json` (multi-task configs are
supported). Minimal example:

```json
{
  "artifact_dir": "artifacts",
  "tasks": [
    {
      "name": "policy_updates",
      "start_url": "http://www.pbc.gov.cn/.../index.html",
      "state_file": "state/policy_state.json",
      "parser": "icrawler.parser_policy",
      "download_types": ["html"]
    }
  ]
}
```

Common commands (CLI flags override config fields):

- `python -m icrawler.pbc_monitor --prefetch-pages` – follow pagination and
  cache every listing page under `artifacts/pages/<task>/`.
- `python -m icrawler.pbc_monitor --dump-structure [path.json]` – build a
  snapshot of entries. Add `--use-cached-pages` to reuse previously cached HTML
  (fall back to network when a page is missing), or `--refresh-pages` to force a
  fresh download.
- `python -m icrawler.pbc_monitor --download-from-structure [path.json]` –
  download attachments from an existing snapshot without recrawling listing
  pages.
- `python -m icrawler.pbc_monitor --run-once` – single monitoring pass that
  fetches listing pages online and downloads new attachments.
- `python -m icrawler.pbc_monitor --fetch-page [page.html]` – save just the
  start page (useful for quick checks; pass `-` to stream to stdout).
- `python -m icrawler.pbc_monitor --dump-from-file <page.html>` – parse an HTML
  file from disk and preview the extracted entries/pagination.

### Typical Workflow

1. Define tasks in `pbc_config.json` (single-task JSON is also accepted).
2. Warm the cache once:
   `python -m icrawler.pbc_monitor --task <name> --prefetch-pages`.
3. Parse and snapshot structure offline:
   `python -m icrawler.pbc_monitor --task <name> --dump-structure --use-cached-pages`.
   Use `--refresh-pages` if you need to bypass cached HTML.
4. Download attachments based on the snapshot:
   `python -m icrawler.pbc_monitor --task <name> --download-from-structure`.
   Alternatively run `--run-once` (or loop without `--run-once`) for an online
   crawl that fetches fresh pages and downloads files in one go.

`output_dir` is optional; when omitted or set to a simple name, files default to
`<artifact_dir>/downloads/<task>/`. Provide an absolute path if you prefer a
custom location.

### Artifacts

All generated files live under `artifact_dir` (default `./artifacts`):

- `pages/` – cached HTML from fetch/snapshot operations.
- `structure/` – JSON snapshots generated by `--dump-structure`.
- `state/` – download history (`state.json` per task).

Relative filenames supplied on the CLI are resolved inside these folders; use an
absolute path to opt out. Adjust the root via `--artifact-dir` or the config.
