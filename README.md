# icrawler

Simple Python-based web crawler that downloads PDF files and converts web
pages to PDF. Provide seed URLs and the crawler will follow links on those
pages, downloading linked PDFs or rendering HTML pages to PDF files.

## Usage

```
python -m icrawler.crawler [--delay N] [--jitter M] <output_dir> <url1> [<url2> ...]
```

`--delay` sets the base pause between requests, and `--jitter` adds up to that
many random seconds so servers are not hammered. Install the dependencies from
`requirements.txt` before running.

## PBC Monitor Quick Start

`icrawler.pbc_monitor` loads tasks from `pbc_config.json` (multi-task configs are
supported). Minimal example:

```json
{
  "artifact_dir": "artifacts",
  "tasks": [
    {
      "name": "policy_updates",
      "start_url": "http://www.pbc.gov.cn/.../index.html",
      "parser": "icrawler.parser_policy"
    }
  ]
}
```

Common commands (CLI flags override config fields):

- `python -m icrawler.pbc_monitor --cache-start-page` – cache the starting page
  HTML (reuses the cached file unless you add `--refresh-pages`; pass `-` to
  stream to stdout).
- `python -m icrawler.pbc_monitor --preview-page-structure` – parse a cached
  page and preview its extracted entries (defaults to the file produced by
  `--cache-start-page`).
- `python -m icrawler.pbc_monitor --cache-listing` – cache every listing page
  under `artifacts/pages/<task>/` (reuses existing cached pages and only fetches
  missing ones unless you pass `--refresh-pages` / `--no-use-cached-pages`).
- `python -m icrawler.pbc_monitor --build-page-structure` – build a full
  listing snapshot. Cached HTML is reused by default.
- `--no-use-cached-pages` / `--refresh-pages` – force retrieval of fresh listing
  pages even when cached copies exist.
- `python -m icrawler.pbc_monitor --download-from-structure` – download
  attachments from an existing snapshot without recrawling listing pages.
- `python -m icrawler.pbc_monitor --run-once` – single monitoring pass that
  fetches listing pages online and downloads new attachments. If the
  starting listing page was cached earlier the same day, the cached copy is
  reused to avoid redundant network requests; older caches trigger a fresh
  fetch automatically. The continuous mode (no `--run-once`) reuses the same
  freshness rule before each iteration, so pages cached today are reused and
  anything older is refreshed when the loop wakes up. After each run, the
  tool logs a per-task summary that includes the number of pages processed,
  entries/documents observed, and how many files were freshly downloaded or
  reused from existing data.

### Typical Workflow

1. Define tasks in `pbc_config.json` (single-task JSON is also accepted).
2. Cache the first page for inspection:
   `python -m icrawler.pbc_monitor --cache-start-page`.
3. Preview that page offline:
   `python -m icrawler.pbc_monitor --preview-page-structure`.
4. Cache the full listing once (reuses existing cached pages by default):
   `python -m icrawler.pbc_monitor --cache-listing`.
5. Build the full structure snapshot:
   `python -m icrawler.pbc_monitor --build-page-structure`.
   Add `--refresh-pages` or `--no-use-cached-pages` if you need to bypass cached HTML.
6. Download attachments based on the snapshot:
   `python -m icrawler.pbc_monitor --download-from-structure`.
   Alternatively run `--run-once` (or loop without `--run-once`) for an online
   crawl that fetches fresh pages and downloads files in one go.

When detail页被下载时，程序会自动解析页面里的附件链接并一并下载，所有对应关系
会写入 `state.json`（含 `local_path`），方便后续追踪。

`output_dir` is optional; when omitted or set to a simple name, files default to
`<artifact_dir>/downloads/<task>/`. Provide an absolute path if you prefer a
custom location.

State tracking and structure snapshots also adopt per-task defaults:

- `state_file` → `<artifact_dir>/downloads/<task>_state.json`
- `structure_file` → `<artifact_dir>/structure/<task>_structure.json`

You can override these via config (`state_file` / `structure_file`) or CLI
(`--state-file`, `--build-page-structure`, `--download-from-structure`).

### Artifacts

All generated files live under `artifact_dir` (default `./artifacts`):

- `pages/` – cached HTML from fetch/snapshot operations.
- `structure/` – JSON snapshots generated by `--build-page-structure`.
- `downloads/` – downloaded files and per-task state JSON.

Relative filenames supplied on the CLI are resolved inside these folders; use an
absolute path to opt out. Adjust the root via `--artifact-dir` or the config.
